{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning with Organized Configuration\n",
    "\n",
    "This notebook demonstrates a clean, organized approach to SSL experiments using class-based configurations.\n",
    "\n",
    "## Key Features:\n",
    "- **Centralized Configuration**: All settings managed through config classes\n",
    "- **Modular Design**: Separate modules for data, models, training, and utilities\n",
    "- **Reproducible Experiments**: Consistent seed setting and configuration management\n",
    "- **Easy Experimentation**: Simple configuration changes for different experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our organized modules\n",
    "from config import (\n",
    "    ExperimentConfig, \n",
    "    get_full_experiment_config, \n",
    "    get_baseline_config, \n",
    "    get_small_experiment_config\n",
    ")\n",
    "from data_utils import DataManager\n",
    "from models import create_autoencoder, create_classifier, print_model_summary\n",
    "from trainers import AutoencoderTrainer, ClassifierTrainer, evaluate_model\n",
    "from utils import (\n",
    "    set_seed, \n",
    "    setup_device, \n",
    "    create_directories, \n",
    "    print_experiment_summary, \n",
    "    visualize_features_tsne\n",
    ")\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup\n",
    "\n",
    "Choose from predefined configurations or create custom ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use predefined configuration\n",
    "config = get_small_experiment_config()  # For quick testing\n",
    "# config = get_full_experiment_config()  # For full experiment\n",
    "# config = get_baseline_config()  # For baseline comparison\n",
    "\n",
    "# Option 2: Create custom configuration\n",
    "# config = ExperimentConfig()\n",
    "# config.experiment_name = \"my_custom_experiment\"\n",
    "# config.data.labeled_size = 3000\n",
    "# config.data.batch_size = 64\n",
    "# config.training.autoencoder_epochs = 30\n",
    "# config.training.classifier_epochs = 25\n",
    "\n",
    "# Validate and display configuration\n",
    "config.validate()\n",
    "print(f\"Experiment: {config.experiment_name}\")\n",
    "print(f\"Labeled samples: {config.data.labeled_size}\")\n",
    "print(f\"Autoencoder epochs: {config.training.autoencoder_epochs}\")\n",
    "print(f\"Classifier epochs: {config.training.classifier_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup reproducibility and device\n",
    "set_seed(config.data.random_seed)\n",
    "device = config.get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create necessary directories\n",
    "create_directories([config.training.checkpoint_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data manager and load datasets\n",
    "data_manager = DataManager(config.data)\n",
    "data_manager.setup_datasets()\n",
    "data_manager.print_data_summary()\n",
    "\n",
    "# Get data loaders\n",
    "labeled_loader, unlabeled_loader, val_loader, test_loader = data_manager.get_data_loaders()\n",
    "\n",
    "print(f\"\\nData loaders created:\")\n",
    "print(f\"  Labeled batches: {len(labeled_loader)}\")\n",
    "print(f\"  Unlabeled batches: {len(unlabeled_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 1: Autoencoder Pre-training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_results = None\n",
    "pretrained_encoder = None\n",
    "\n",
    "if config.training.autoencoder_epochs > 0:\n",
    "    print(\"=== Phase 1: Autoencoder Pre-training ===\")\n",
    "    \n",
    "    # Create autoencoder\n",
    "    autoencoder = create_autoencoder(config.model).to(device)\n",
    "    print_model_summary(autoencoder, \"Autoencoder\")\n",
    "    \n",
    "    # Train autoencoder\n",
    "    autoencoder_trainer = AutoencoderTrainer(\n",
    "        autoencoder, \n",
    "        config.training, \n",
    "        device,\n",
    "        config.training.checkpoint_dir\n",
    "    )\n",
    "    \n",
    "    autoencoder_results = autoencoder_trainer.train(unlabeled_loader, val_loader)\n",
    "    pretrained_encoder = autoencoder.encoder\n",
    "    \n",
    "    print(f\"\\nAutoencoder training completed!\")\n",
    "    print(f\"Best validation loss: {autoencoder_results['best_val_loss']:.6f}\")\n",
    "else:\n",
    "    print(\"=== Skipping autoencoder pre-training ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Learned Features (if autoencoder was trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pretrained_encoder is not None:\n",
    "    print(\"=== Visualizing learned features ===\")\n",
    "    visualize_features_tsne(\n",
    "        autoencoder, \n",
    "        val_loader, \n",
    "        device, \n",
    "        max_samples=2000,\n",
    "        save_path=os.path.join(config.training.checkpoint_dir, \"tsne_features.png\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phase 2: Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Phase 2: Classifier Training ===\")\n",
    "\n",
    "# Create classifier\n",
    "classifier = create_classifier(config.model, pretrained_encoder).to(device)\n",
    "print_model_summary(classifier, \"Classifier\")\n",
    "\n",
    "# Train classifier\n",
    "classifier_trainer = ClassifierTrainer(\n",
    "    classifier, \n",
    "    config.training, \n",
    "    device,\n",
    "    config.training.checkpoint_dir\n",
    ")\n",
    "\n",
    "classifier_results = classifier_trainer.train(labeled_loader, val_loader)\n",
    "\n",
    "print(f\"\\nClassifier training completed!\")\n",
    "print(f\"Best validation accuracy: {classifier_results['best_val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Final Evaluation ===\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy = evaluate_model(classifier, test_loader, device)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Print comprehensive experiment summary\n",
    "results = {\n",
    "    'Best Validation Accuracy': classifier_results['best_val_acc'],\n",
    "    'Test Accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "if autoencoder_results:\n",
    "    results['Best Autoencoder Val Loss'] = autoencoder_results['best_val_loss']\n",
    "\n",
    "print_experiment_summary(config, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Classifier training history\n",
    "history = classifier_results['history']\n",
    "epochs = range(1, len(history['train_acc']) + 1)\n",
    "\n",
    "axes[0].plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n",
    "axes[0].plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n",
    "axes[0].set_title('Classifier Training History')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "axes[1].plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "axes[1].set_title('Classifier Loss History')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Autoencoder history (if available)\n",
    "if autoencoder_results:\n",
    "    ae_history = autoencoder_results['history']\n",
    "    ae_epochs = range(1, len(ae_history['train_loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(ae_epochs, ae_history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.plot(ae_epochs, ae_history['val_loss'], 'r-', label='Validation Loss')\n",
    "    plt.title('Autoencoder Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Reconstruction Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Configuration Comparison Example\n",
    "\n",
    "Demonstrate how easy it is to run different experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare different labeled data sizes\n",
    "def compare_labeled_sizes():\n",
    "    \"\"\"Compare performance with different amounts of labeled data.\"\"\"\n",
    "    labeled_sizes = [1000, 2000, 5000]\n",
    "    results = []\n",
    "    \n",
    "    for size in labeled_sizes:\n",
    "        print(f\"\\n=== Testing with {size} labeled samples ===\")\n",
    "        \n",
    "        # Create configuration\n",
    "        test_config = get_small_experiment_config()\n",
    "        test_config.data.labeled_size = size\n",
    "        test_config.experiment_name = f\"ssl_labeled_{size}\"\n",
    "        \n",
    "        # Quick setup and training (simplified)\n",
    "        set_seed(test_config.data.random_seed)\n",
    "        device = test_config.get_device()\n",
    "        \n",
    "        # Setup data\n",
    "        dm = DataManager(test_config.data)\n",
    "        dm.setup_datasets()\n",
    "        labeled_loader, _, val_loader, test_loader = dm.get_data_loaders()\n",
    "        \n",
    "        # Train classifier (skip autoencoder for speed)\n",
    "        classifier = create_classifier(test_config.model).to(device)\n",
    "        trainer = ClassifierTrainer(classifier, test_config.training, device)\n",
    "        train_results = trainer.train(labeled_loader, val_loader)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_acc = evaluate_model(classifier, test_loader, device)\n",
    "        \n",
    "        results.append({\n",
    "            'labeled_size': size,\n",
    "            'val_acc': train_results['best_val_acc'],\n",
    "            'test_acc': test_acc\n",
    "        })\n",
    "        \n",
    "        print(f\"Results: Val Acc: {train_results['best_val_acc']:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run comparison\n",
    "# comparison_results = compare_labeled_sizes()\n",
    "print(\"Comparison function defined. Uncomment the line above to run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This organized approach provides:\n",
    "\n",
    "1. **Clean Configuration Management**: All experiment settings in one place\n",
    "2. **Modular Design**: Easy to modify individual components\n",
    "3. **Reproducible Experiments**: Consistent seed setting and configuration\n",
    "4. **Easy Experimentation**: Simple configuration changes for different setups\n",
    "5. **Professional Code Structure**: Follows software engineering best practices\n",
    "\n",
    "### Key Benefits:\n",
    "- **Maintainable**: Easy to understand and modify\n",
    "- **Extensible**: Simple to add new models, datasets, or training strategies\n",
    "- **Reproducible**: Consistent results across runs\n",
    "- **Configurable**: Easy to run different experiments\n",
    "- **Professional**: Clean, organized code structure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
